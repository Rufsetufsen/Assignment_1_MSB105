% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  a4paper,
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{british}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\ifLuaTeX
  \usepackage[english]{selnolig} % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Is reproducibility good enough?},
  pdfauthor={Kristoffer Tufta},
  pdflang={en-GB},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Is reproducibility good enough?}
\author{Kristoffer Tufta}
\date{Friday 26 Sep, 2025}

\begin{document}
\maketitle
\begin{abstract}
The scientific community has in recent years faced scrutiny due to a
lack of reproducibility in published studies. This paper discuss the
implications of this reproducibility crisis, angled especially towards
the fields of economics, and the path from here on out. The focus of
this paper is not just on the technical hurdles between the current
status and full reproducibility, but also the cultural aspects that lead
us there. As it turns out, positive outcome biases and type 1 errors
cannot be easily solved by new technology, and change needs to happen on
a more institutional level. Further, I discuss how access to powerful
computable documents such as Quarto documents in RStudio help us on the
way towards reproducibility and robust science.
\end{abstract}


\section{Introduction}\label{introduction}

This mini-paper is based on the university lecture ``Robust and Reliable
Science'' (Gjestland, 2024) which highlights the need for
reproducibility in research.

In recent years, scientific research has come under scrutiny due to
concerns about the reproducibility of published findings (McNutt, 2014,
p. 229). The term \emph{reproducibility} refers to the ability of
independent researchers to obtain the same scientific results when using
the original data and the same scientific methods. Reproducibility is
also a necessary condition for \emph{replicable} research, which
involves

\begin{quote}
``The confirmation of results and conclusions from one study obtained
independently in another.'' (Jasny et al., 2011 p 1225).
\end{quote}

Both reproducibility and replicability are central to the credibility of
empirical research, yet evidence suggests that quite a large share of
published research across many academic disciplines fail to meet these
standards.

\section{The Reproducibility Crisis}\label{the-reproducibility-crisis}

The reproducibility crisis first became visible in the academic fields
of psychology and medicine before it reached economics. The open Science
Collaboration (2015) found that only 36 percent of their tested
psychology studies were replicable. In preclinical cancer research,
Begley \& Ellis (2012) showed that only a small fraction of results
could be reproduced and/or scientifically validated. Simmons et al.
(2011) presents how easy it is to accumulate statistically significant
evidence for demonstrably false hypotheses. Freese \& Peterson (2017)
shows that the same is the case in political science.

The field of Economics has not been spared either. Camerer et al. (2016)
attempted to replicate 18 experimental studies and found that only 61
percent showed effects in the same direction, and even then the average
replicated effect size was only 66 percent of the original. Miguel
(2021) emphasises that selective publishing and inacessible datasets
create barriers to replication. The reproducibility issue seems to be a
composite of both inacessible data and statistics, but also culture and
incentives. ``Significant'' results get rewarded with publishing, while
null-findings get ignored and end up in a desk drawer somewhere.
Publication bias incentivizes cherry-picking of results to obtain
p-values below 0,05 (Miguel, 2021 pp 194). The ``positive outcome bias''
is not only distorting the scientific literature, but it can also
pressure scientists to fabricate and falsify their data (Fanelli, 2012).

For an academic discipline like economics or political science where
evidence is used to justify political reforms, this can have lasting
real-world consequences for millions of people, and it creates a real
credibility problem for the field.

\section{Principles of Robust and Reliable
Science}\label{principles-of-robust-and-reliable-science}

\begin{quote}
``Robust and reliable research is the foundation of all scientific
development and progress, which depends critically on the ability of
investigators to build on prior work.'' ({`{NSF Dear Colleague
Letter}'}, n.d.).
\end{quote}

Bollen et al. (2015) view robust scientific findings as ones that are
reproducible, replicable, and generalizable.

According to Bollen et al. (2015) this means that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A researcher should be able to duplicate the results from a prior
  study with the same method and materials as the original researcher
  (reproducibility).
\item
  A researcher should be able to duplicate the results from a prior
  study with the same method and collect new results using different
  data (replicability).
\item
  The results from the research should be able to be generalized across
  other similar contexts (generalizability).
\end{enumerate}

So how do we achieve this? I'd like to summarize the contents from the
lecture (with some additional sources) and structure them into three
principles:

\begin{itemize}
\tightlist
\item
  Transparency: Gathered data and code must be shared whenever possible
  (Gentleman \& Temple Lang, 2007; Stodden, 2010).
\item
  More robust methods: Reliance on p-values alone is insufficient. ``All
  roads lead to Rome'', and almost as many lead to statistical
  significance. One solution could be preregistration (defining the
  data-collection and methods ahead of time), but this is not always
  possible to do in observational fields such as economics (Gelman \&
  Loken, 2013).
\item
  Cultural change: incentives should reward replicability and openness.
  Normalize publishing research with negative outcomes to limit the
  effects of positive outcome bias (Nosek et al., 2012).
\end{itemize}

The effort put into Klein et al. (2018) shows that collaboration and
openness about the scientific process can raise standards. Still,
without changes in publishing policies and funding criteria, these
efforts risk staying a minority.

\section{Implications for Data Science in
Economics}\label{implications-for-data-science-in-economics}

For economists, reproducibility is a safeguard against policy failure.
Research influences taxes, welfare systems and development aid. If
results remain non-reproducible, the advice risks being wrong. Deaton
(2010) argues that poorly designed studies in economics lead to
misleading conclusions, sometimes with direct consequences for
policymaking.

Still, there is hope on the horizon. Economics journals now increasingly
require data and code archives, and the general consensus in academic
circles seems to be trending towards more transparency and
reproducibility in research, even on an institutional level (Christensen
\& Miguel, 2018, p. 969). Lack of access to computer programs used to be
a big hurdle when it came to replicating economic research (Dewald et
al., 1986, p. 588) but now, open source tools like Quarto documents and
RStudio exist as a way for written code and datasets to be fully
integrated into the finished papers, easing the path towards
reproducibility (Bauer \& Landesvatter, n.d.). This allows peers to
inspect the full workflow, not just the end product, and whatever
code/data the original researcher made the effort to clean up and share.
However, technical tools do not solve the full issue with publication
bias and type 1 errors, and reproducibility still depends on software
versions (e.g R-version, packages used, hardware etc).

\section{Discussion}\label{discussion}

Should replication be the norm in science? In theory: yes, but in
practice, replication of every single scientific study would be
unrealistic. There are simply too many variables at play that could
affect the end result of the study, even if instructions are followed to
a tee. However, I still believe that it should be possible to \emph{try}
to replicate a study. In my opinion, \emph{it is} too much to ask that
the scientific community blindly accepts results without being able to
test the theories themselves. I would therefore say that
\emph{reproducibility} should be the norm. Even for researchers who care
less about idealism and openness, working reproducibly can be beneficial
(Markowetz, 2015).

So how do we get there? For one, making powerful tools like RStudio and
Quarto documents free, open source and accessible to everyone is a big
step. Including full datasets, live code blocks and the end result in
the same computable Quarto document makes the process transparent and
easy to reproduce, given adherence to robust research principles (like
including version numbers and all packages used).

\section{Conclusion}\label{conclusion}

The reproducibility crisis shows that science suffers from weaknesses
both of cultural and technical origin. Technical barriers such as
missing and/or inaccessible data and poor documentation undermine
reproducibility of research. Cultural barriers like publication bias, a
preference for ``positive'' findings and a lack of incentives for
replication reinforce this problem.

For economics and data science, these weak spots are especially
consequential. Research in these fields directly shape public policy and
decisions on a macro level. If the underlying evidence cannot be
reproduced, it cannot be fully trusted either. Reproducibility should
therefore be treated as a baseline, while replication remains a gold
standard to test whether findings are general and robust (although
actual replication of research seems to be a rarity).

New tools like Quarto and RStudio make research workflows transparent
and easier to inspect or peer-review, but no technical tool can resolve
the underlying incentive problems in modern research. Change also needs
to happen on an institutional level. Scientific journals and
universities must value and prioritize openness and strive towards
replicability if robust and reliable science is to become a norm.

\section{Further reading}\label{further-reading}

For readers interested in a broader perspective on the topic, I've
compiled a short list of several other contributions of note. Ioannidis
(2005) provides an argument on how most research claims are likely
false, and the implications of this. For researchers wishing to take the
first steps towards robust an reliable science, Ellis \& Leek (2017)
provides guidelines and general principles on sharing data for
collaboration. Munafò et al. (2017) outline a ``manifesto'' for
reproducible science across disciplines.

\section{Software Citations}\label{software-citations}

\begin{itemize}
\tightlist
\item
  R version 4.5.1, with loaded packages: ``Base'', ``datasets'',
  ``graphics'', ``grDevices'', ``methods'', ``stats'', R Core Team
  (2025). \_R: A Language and Environment for Statistical Computing\_. R
  Foundation for Statistical Computing, Vienna, Austria.
  \textless https://www.R-project.org/\textgreater.
\item
  RStudio 2025.9.0.387, release ``Cucumberleaf Sunflower''. Posit team
  (2025). RStudio: Integrated Development Environment for R. Posit
  Software, PBC, Boston, MA. URL http://www.posit.co/.
\item
  Running under: Windows 11 x64 (build 26100).
\end{itemize}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bauer2023}
Bauer, P. C., \& Landesvatter, C. (n.d.). \emph{Writing a reproducible
paper with RStudio and quarto}.
\url{https://doi.org/10.31219/osf.io/ur4xn}

\bibitem[\citeproctext]{ref-begley2012}
Begley, C. G., \& Ellis, L. M. (2012). Raise standards for preclinical
cancer research. \emph{Nature}, \emph{483}(7391), 531--533.
\url{https://doi.org/10.1038/483531a}

\bibitem[\citeproctext]{ref-bollen2015}
Bollen, K., Cacioppo, J. T., Kaplan, R. M., Krosnick, J. A., \& Olds, J.
L. (2015). \emph{Social, behavioral, and economic sciences perspectives
on robust and reliable science}. National Science Foundation.

\bibitem[\citeproctext]{ref-camerer2016}
Camerer, C. F., Dreber, A., Forsell, E., Ho, T.-H., Huber, J.,
Johannesson, M., Kirchler, M., Almenberg, J., Altmejd, A., Chan, T.,
Heikensten, E., Holzmeister, F., Imai, T., Isaksson, S., Nave, G.,
Pfeiffer, T., Razen, M., \& Wu, H. (2016). Evaluating replicability of
laboratory experiments in economics. \emph{Science}, \emph{351}(6280),
1433--1436. \url{https://doi.org/10.1126/science.aaf0918}

\bibitem[\citeproctext]{ref-christensen2018}
Christensen, G., \& Miguel, E. (2018). Transparency, reproducibility,
and the credibility of economics research. \emph{Journal of Economic
Literature}, \emph{56}(3), 920--980.
\url{https://doi.org/10.1257/jel.20171350}

\bibitem[\citeproctext]{ref-opensciencecollaboration2015}
Collaboration, O. S. (2015). Estimating the reproducibility of
psychological science. \emph{Science}, \emph{349}(6251).

\bibitem[\citeproctext]{ref-deaton2010}
Deaton, A. (2010). Instruments, randomization, and learning about
development. \emph{Journal of Economic Literature}, \emph{48}(2),
424--455. \url{https://www.jstor.org/stable/20778731}

\bibitem[\citeproctext]{ref-dewald1986b}
Dewald, W. G., Thursby, J. G., \& Anderson, R. G. (1986). Replication in
empirical economics: The journal of money, credit and banking project.
\emph{The American Economic Review}, \emph{76}(4), 587--603.

\bibitem[\citeproctext]{ref-ellis2017}
Ellis, S. E., \& Leek, J. T. (2017). \emph{How to share data for
collaboration}.

\bibitem[\citeproctext]{ref-fanelli2012}
Fanelli, D. (2012). Negative results are disappearing from most
disciplines and countries. \emph{Scientometrics}, \emph{90}(3),
891--904. \url{https://doi.org/10.1007/s11192-011-0494-7}

\bibitem[\citeproctext]{ref-freese2017}
Freese, J., \& Peterson, D. (2017). Replication in social science.
\emph{Annual Review of Sociology}, \emph{43}, 147--165.
\url{https://www.jstor.org/stable/44863227}

\bibitem[\citeproctext]{ref-gelman2013}
Gelman, A., \& Loken, E. (2013). \emph{The garden of forking paths: Why
multiple comparisons can be a problem, even when there is no
{``}{fi}shing expedition{''} or {``}p-hacking{''} and the research
hypothesis was posited ahead of time}.

\bibitem[\citeproctext]{ref-gentleman2007b}
Gentleman, R., \& Temple Lang, D. (2007). Statistical analyses and
reproducible research. \emph{Journal of Computational and Graphical
Statistics}, \emph{16}(1), 1--23.

\bibitem[\citeproctext]{ref-gjestland2024}
Gjestland, A. (2024). \emph{Data science - robust and reliable science}.
\url{https://msb105.netlify.app/introduction/reproducibility/rr-science_h24\#/}

\bibitem[\citeproctext]{ref-ioannidis2005b}
Ioannidis, J. P. A. (2005). Why most published research findings are
false. \emph{PLOS Medicine}, \emph{2}(8), e124.

\bibitem[\citeproctext]{ref-jasny2011b}
Jasny, B. R., Chin, G., Chong, L., \& Vignieri, S. (2011). Again, and
again, and again. \emph{Science}, \emph{334}(6060), 1225--1225.

\bibitem[\citeproctext]{ref-klein2018b}
Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Reginald B.
Adams, Jr., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník,
Š., \& al., et. (2018). Many labs 2: Investigating variation in
replicability across samples and settings. \emph{Advances in Methods and
Practices in Psychological Science}, \emph{1}(4), 443--490.

\bibitem[\citeproctext]{ref-markowetz2015a}
Markowetz, F. (2015). Five selfish reasons to work reproducibly.
\emph{Genome Biology}, \emph{16}(1), 274.

\bibitem[\citeproctext]{ref-mcnutt2014}
McNutt, M. (2014). Reproducibility. \emph{Science}, \emph{343}(6168),
229--229.

\bibitem[\citeproctext]{ref-miguel2021}
Miguel, E. (2021). Evidence on Research Transparency in Economics.
\emph{Journal of Economic Perspectives}, \emph{35}(3), 193--214.
\url{https://doi.org/10.1257/jep.35.3.193}

\bibitem[\citeproctext]{ref-munafo2017}
Munafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers,
C. D., Percie Du Sert, N., Simonsohn, U., Wagenmakers, E.-J., Ware, J.
J., \& Ioannidis, J. P. A. (2017). A manifesto for reproducible science.
\emph{Nature Human Behaviour}, \emph{1}(1), 0021.
\url{https://doi.org/10.1038/s41562-016-0021}

\bibitem[\citeproctext]{ref-nosek2012}
Nosek, B. A., Spies, J. R., \& Motyl, M. (2012). Scientific utopia: II.
Restructuring incentives and practices to promote truth over
publishability. \emph{Perspectives on Psychological Science},
\emph{7}(6), 615--631. \url{https://doi.org/10.1177/1745691612459058}

\bibitem[\citeproctext]{ref-nsfDear}
{NSF Dear Colleague Letter}: {Robust} and {Reliable Research} in the
{Social}, {Behavioral}, and {Economic Sciences}. (n.d.). In \emph{AABA}.

\bibitem[\citeproctext]{ref-simmons2011}
Simmons, J. P., Nelson, L. D., \& Simonsohn, U. (2011). False-positive
psychology: Undisclosed flexibility in data collection and analysis
allows presenting anything as significant. \emph{Psychological Science},
\emph{22}(11), 13591366.

\bibitem[\citeproctext]{ref-stodden2010}
Stodden, V. (2010). The scientific method in practice: Reproducibility
in the computational sciences. \emph{SSRN Electronic Journal}.
\url{https://doi.org/10.2139/ssrn.1550193}

\end{CSLReferences}




\end{document}
