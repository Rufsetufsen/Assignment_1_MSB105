---
title: "Is reproducibility good enough?"
author: "Kristoffer Tufta"
date: last-modified
date-format: "dddd D MMM, YYYY"
csl: apa7.csl
lang: en-GB
citeproc: true
papersize: a4
number-sections: true
format:
  html: default
  typst: default
  pdf: 
    documentclass: article
    keep-tex: true
    papersize: A4
    fig-pos: "H"
bibliography: 
  - reproducibility_KT.bib
  - reproducibility_ag.bib
abstract: "The scientific community has in recent years faced scrutiny due to a lack of reproducibility in published studies. This paper iscuss the implications of this reproducibility crisis, angled especially towards the fields of economics, and the path from here on out. The focus of this paper is not just on the technical hurdles between the current status and full reproducibility, but also the cultural aspects that lead us there. As it turns out, positive outcome biases and type 1 errors cannot be easily solved by new technology, and change needs to happen on a more institutional level. Further, I discuss how access to powerful computable documents such as Quarto documents in RStudio help us on the way towards reproducibility and robust science."
editor: 
  markdown: 
    wrap: sentence
---

## Introduction

This mini-paper is based on the university lecture "Robust and Reliable Science" [@gjestland2024] which highlights the need for reproducibility in research.

In recent years, scientific research has come under scrutiny due to concerns about the reproducibility of published findings [@mcnutt2014 page 229].
The term "*reproducibility*" refers to the ability of independent researchers to obtain the same scientific results when using the original data and scientific methods.
Reproducibility is also a necessary condition for *replicable* research, which involves *"The confirmation of results and conclusions from one study obtained independently in another."* [@jasny2011b p 1225].

Both reproducibility and replicability are central to the credibility of empirical research, yet evidence suggests that quite a large share of published research across many academic disciplines fail to meet these standards.

## The Reproducibility Crisis

The reproducibility crisis first became visible in the academic fields of psychology and medicine before it reached economics.
The open Science @opensciencecollaboration2015 found that only 36 percent of their tested psychology studies were replicable.
In preclinical cancer research, @begley2012 showed that only a small fraction of results could be reproduced and/or scientifically validated.
@simmons2011 presents how easy it is to accumulate statistically significant evidence for demonstrably false hypotheses.
@freese2017 shows that the same is the case in political science.

The field of Economics has not been spared either.
@camerer2016 attempted to replicate 18 experimental studies and found that only 61 percent showed effects in the same direction, and even then the average replicated effect size was only 66 percent of the original.
@miguel2021 emphasises that selective publishing and inacessible datasets create barriers to replication.
The reproducibility issue seems to be a composite of both inacessible data and statistics, but also culture and incentives.
"Significant" results get rewarded with publishing, while null-findings get ignored and end up in a desk drawer somewhere.
Publication bias incentivizes cherry-picking of results to obtain p-values below 0,05 [@miguel2021 pp 194].
The "positive outcome bias" is not only distorting the scientific literature, but it can also pressure scientists to fabricate and falsify their data [@fanelli2012].

For an academic discipline like economics or political science where evidence is used to justify political reforms, this can have lasting real-world consequences for millions of people, and it creates a real credibility problem for the field.

## Principles of Robust and Reliable Science

*"Robust and reliable research is **the foundation of all scientific development** and progress, which depends critically on the ability of investigators to build on prior work."* [@gjestland2024].

So what is robust and reliable research?
I choose to use the same definition as @bollen2015 who: *"view robust scientific* *findings as ones that are reproducible, replicable, and generalizable."*

This means that:

1.  A researcher should be able to duplicate the results from a prior study with the same method and materials as the original researcher (reproducibility).
2.  A researcher should be able to duplicate the results from a prior study with the same method and collect new results using different data (replicability).
3.  The results from the research should be able to be generalized across other similar contexts (generalizability).

@gjestland2024

So how do we achieve this?
I'd like to summarize the contents from the lecture (with some additional sources) and structure them into three principles:

-   Transparency: Gathered data and code must be shared whenever possible @stodden2010 ; @gentleman2007b .
-   More robust methods: Reliance on p-values alone is insufficient. "All roads lead to Rome", and almost as many lead to statistical significance. One solution could be preregistration (defining the data-collection and methods ahead of time), but this is not always possible to do in observational fields such as economics @gelman2013.
-   Cultural change: incentives should reward replicability and openness. Normalize publishing research with negative outcomes to limit the effects of positive outcome bias @nosek2012.

The effort put into [@klein2018b] shows that collaboration and openness about the scientific process can raise standards.
Still, without changes in publishing policies and funding criteria, these efforts risk staying a minority.

## Implications for Data Science in Economics

For economists, reproducibility is a safeguard against policy failure.
Research influences taxes, welfare systems and development aid.
If results remain non-reproducible, the advice risks being wrong.
@deaton2010 argues that poorly designed studies in economics lead to misleading conclusions, sometimes with direct consequences for policymaking.

Still, there is hope on the horizon.
Economics journals now increasingly require data and code archives, and the general consensus in academic circles seems to be trending towards more transparency and reproducibility in research, even on an institutional level [@christensen2018 p.969].
Lack of access to computer programs used to be a big hurdle when it came to replicating economic research [@dewald1986b p.588] but now, open source tools like Quarto documents and RStudio now exist as a way for written code and datasets to be fully integrated into the finished papers, easing the path towards reproducibility [@bauer2023].
This allows peers to inspect the full workflow, not just the end product, and whatever code/data the original researcher made the effort to clean up and share.
However, technical tools do not solve the full issue with publication bias and type 1 errors, and reproducibility still depends on software versions (e.g R-version, packages used, hardware etc).

## Discussion

Should replication be the norm in science?
In theory: yes, but in practice, replication of every single scientific study would be unrealistic.
There are simply too many variables at play that could affect the end result of the study, even if instructions are followed to a tee.
However, I still believe that it should be possible to *try*Â to replicate a study.
In my opinion, *it is* too much to ask that the scientific community blindly accepts results without being able to test the theories themselves.
I would therefore say that *reproducibility* should be the norm.
Even for researchers who care less about idealism and openness, working reproducibly can be beneficial [@markowetz2015a].

So how do we get there?
For one, making powerful tools like RStudio and Quarto documents free, open source and accessible to everyone is a big step.
Including full datasets, live code blocks and the end result in the same computable Quarto document makes the process transparent and easy to reproduce, given adherance to robust research principles (like including version numbers and all packages used).

## Conclusion

The reproducibility crisis shows that science suffers from weaknesses both of cultural and technical origin.
Technical barriers such as missing and/or inacessible data and poor documentation undermine reproducibility of research.
Cultural barriers like publication bias, a preference for "positive" findings and a lack of incentives for replication reinforce this problem.

For economics and data science, these weak spots are especially consequential.
Research in these fields directly shape public policy and decisions on a macro level.
If the underlying evidence cannot be reproduced, it cannot be fully trusted either.
Reproducibility should therefore be treated as a baseline, while replication remains a gold standard to test whether findings are general and robust (although actual replication of research seems to be a rarity).

New tools like Quarto and RStudio make research workflows transparent and easier to inspect or peer-review, but no technical tool can resolve the underlying incentive problems in modern research.
Change also needs to happen on an institutional level.
Scientific journals and universities must value and prioritize openness and strive towards replicability if robust and reliable science is to become a norm.

## Further reading

For readers interested in a broader perspective on the topic, I've compiled a short list of several other contributions of note.
@ioannidis2005b provides an argument on how most research claims are likely false, and the implications of this.
For researchers wishing to take the first steps towards robust an reliable science, @ellis2017 provides guidelines and general principles on sharing data for collaboration.
@munafo2017 outline a "manifesto" for reproducible science across disciplines.

## Software Citations

-   R version 4.5.1, with loaded packages: "Base", "datasets", "graphics", "grDevices", "methods", "stats", R Core Team (2025). \_R: A Language and Environment for Statistical Computing\_. R Foundation for Statistical Computing, Vienna, Austria. \<https://www.R-project.org/\>.
-   RStudio 2025.9.0.387, release "Cucumberleaf Sunflower". Posit team (2025). RStudio: Integrated Development Environment for R. Posit Software, PBC, Boston, MA. URL http://www.posit.co/.
-   Running under: Windows 11 x64 (build 26100).

## References
