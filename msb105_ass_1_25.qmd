---
title: "Is reproducibility good enough?"
author: "Kristoffer Tufta"
date: last-modified
date-format: "dddd D MMM, YYYY"
csl: apa7.csl
lang: en-GB
format:
  html: default
  typst:
    papersize: a4
  pdf: 
    documentclass: article
    number-sections: true
    keep-tex: true
    papersize: A4
    fig-pos: "H"
bibliography: reproducibility.bib
nocite: '@*'
abstract: "A very short abstract. Put the abstract text here. One or two paragraphs summarising what follows below."
editor: 
  markdown: 
    wrap: sentence
---

## Introduction

This mini-paper is based on the university lecture "Robust and Reliable Science" [@gjestland2024] which highlights the need for reproducibility in research.

In recent years, scientific research has come under scrutiny due to concerns about the reproducibility of published findings [@mcnutt2014 page 229].
The term "*reproducibility*" refers to the ability of independent researchers to obtain the same scientific results when using the original data and scientific methods.
Reproducibility is also a necessary condition for *replicable* research, which involves *"The confirmation of results and conclusions from one study obtained independently in another."* [@jasny2011b p 1225].

Both reproducibility and replicability are central to the credibility of empirical research, yet evidence suggests that quite a large share of published research across many academic disciplines fail to meet these standards.

## The Reproducibility Crisis

The reproducibility crisis first became visible in the academic fields of psychology and medicine before it reached economics.
The open Science @opensciencecollaboration2015 found that only 36 percent of their tested psychology studies were replicable.
In preclinical cancer research, @begleyRaiseStandardsPreclinical2012 showed that only a small fraction of results could be reproduced and/or scientifically validated.

The field of Economics has not been spared either.
@camerer2016 attempted to replicate 18 experimental studies and found that only 61 percent showed effects in the same direction, and even then the average replicated effect size was only 66 percent of the original.
@miguel2021 emphasises that selective publishing and inacessible datasets create barriers to replication.
The reproducibility issue seems to be a composite of both inacessible data and statistics, but also culture and incentives.
"Significant" results get rewarded with publishing, while null-findings get ignored and end up in a desk drawer somewhere.
Publication bias incentivizes cherry-picking of results to obtain p-values below 0,05 [@miguel2021 pp 194].
The "positive outcome bias" is not only distorting the scientific literature, but it can also pressure scientists to fabricate and falsify their data [@fanelli2012].

For an academic discipline like economics where evidence is used to justify political reforms, this can have lasting real-world consequences for millions of people, and it creates a real credibility problem for the field.

## Principles of Robust and Reliable Science

*"Robust and reliable research is **the foundation of all scientific development** and progress, which depends critically on the ability of investigators to build on prior work."* [@gjestland2024].

So what is robust and reliable research?
The wise man pacdoesn't feel the need to invent the wheel twice, so I choose to use the same definition as @bollen2015: [they] *"view robust scientific* *findings as ones that are reproducible, replicable, and generalizable."*

This means that:

1.  A researcher should be able to duplicate the results from a prior study with the same method and materials as the original researcher (reproducibility).
2.  A researcher should be able to duplicate the results from a prior study with the same method and collect new results using different data (replicability).
3.  The results from the research should be able to be generalized across other similar contexts (generalizability).

@gjestland2024

So how do we achieve this?
I'd like to condense the contents from the lecture into three principles:

-   Transparency: Gathered data and code must be shared whenever possible @stodden2010 ; @gentleman2007b .
-   More robust methods: Reliance on p-values alone is insufficient. "All roads lead to Rome", and many roads lead to statistical significance. One solution could be preregistration (defining the data-collection and methods ahead of time), but this is not always possible to do in observational fields such as economics @gelman.
-   Cultural change: incentives should reward replicability and openness. Normalize publishing research with negative outcomes to limit the effects of positive outcome bias.

The effort put into [@klein2018b] shows that collaboration and openness about the scientific process can raise standards.
Still, without changes in publishing policies and funding criteria, these efforts risk staying a minority.

## Implications for Data Science in Economics

For economists, reproducibility is a safeguard against policy failure.
Research influences taxes, welfare systems and development aid.
If results remain non-reproducible, the advice risks being wrong.
@deaton2010 argues that poorly designed studies in economics lead to misleading conclusions, sometimes with direct consequences for policymaking.

Still, there is hope on the horizon.
Economics journals now increasingly require data and code archives, and the general consensus in academic circles seems to be trending towards more transparency and reproducibility in research, even on an institutional level [@christensen2018 p.969].
Lack of access to computer programs used to be a big hurdle when it came to replicating economic research [@dewald1986b p.588] but now, open source tools like Quarto documents and RStudio now exist as a way for written code and datasets to be fully integrated into the finished papers, easing the path towards reproducibility [@bauer2023].
This allows peers to inspect the full workflow, not just the end product, and whatever code/data the original researcher chose to disclose.
However, technical tools do not solve the full issue with publication bias and type 1 errors, and reproducibility still depends on software versions (e.g R-version, packages used, hardware etc).

## Discussion

Should replication be the norm in science?
In theory: yes, but in practice, replication of every single scientific study would be unrealistic.
There are simply too many variables at play that could affect the end result of the study, even if the instructions are followed to a tee.
However, I still believe that it should be possible to *try* to replicate a study.
It is too much to ask that the scientific community blindly accepts results without being able to test the theories themselves.
I would therefore say that *reproducibility* should be the norm.

So how do we get there?
For one, making powerful tools like RStudio and Quarto documents free, open source and accessible to everyone is a big step.
Including full datasets, live code blocks and the end result in the same living quarto document makes the process transparent and easy to reproduce, given adherance to robust research principles (like including version numbers and all packages used).

## Conclusion

The reproducibility crisis shows that science suffers from weaknesses both of cultural and technical origin.
Technical barriers such as missing and/or inacessible data and poor documentation undermine reproducibility of research.
Cultural barriers like publication bias, a preference for "positive" findings and a lack of incentives for replication reinforce this problem.

For economics and data science, these weak spots are especially consequential.
Research in these fields directly shape public policy and decisions on a macro level.
If the underlying evidence cannot be reproduced, it cannot be fully trusted either.
Reproducibility should therefore be treated as a baseline, while replication remains a gold standard to test whether findings are general and robust (although actual replication of research seems to be a rarity).

New tools like Quarto and RStudio make research workflows transparent and easier to inspect or peer-review, but no technical tool can resolve the underlying incentive problems in modern research.
Change also needs to happen on an institutional level.
Scientific journals and universities must value and prioritize openness and strive towards replicability if robust and reliable science is to become a norm.

## Literature review / Further reading(?)

Smart stuff from others about the topic.

Use a least 20 citations, a least 5 of them must be new (not from the provided .bib file).

Use both in-line and normal citations.

Example:

@gentleman2005 argues that bla bla bla.
On the other hand it's claimed that bla bla [@barbalorenaa.2018; @bartlett2008].

## Discussion of the reseach question

-   Should replicability be the norm or is this to much to ask for now?
-   Can Quarto documents help with reproducibility?
-   What problems remains and how can these be solved?

## Software used

R version 4.5.1

Arnstein går gjennom korleis sitere package 17.09.

## References

and

-   Version number and reference to packages used(
-   R version used

## 
